import json
import streamlit as st
from transformers import pipeline
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import HuggingFacePipeline

# ========== 1) JSON íŒŒì¼ ì½ê¸° ==========
with open("qa_data.json", "r", encoding="utf-8") as f:
    qa_list = json.load(f)

docs = []
for qa in qa_list:
    q = qa["question"]
    a = qa["answer"]["2_planning_for_AI_usage"]["purpose"]
    e = qa["answer"]["3_prompt_and_feedback_process"]["chatGPT_response"]["analysis_summary"][0]
    full_text = f"Q: {q}\nA: {a}\nExplanation: {e}"
    docs.append(Document(page_content=full_text))

# ========== 2) ë²¡í„°ìŠ¤í† ì–´ ì„¸íŒ… ==========
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
faiss = FAISS.from_documents(docs, embeddings)
retriever = faiss.as_retriever()

# ========== 3) GPT-Neo ëª¨ë¸ ì„¤ì • ==========
generator = pipeline("text-generation", model="EleutherAI/gpt-neo-2.7B", max_length=512, do_sample=True, temperature=0.8)
llm = HuggingFacePipeline(pipeline=generator)

template = """
ë„ˆëŠ” í€´ì¦ˆ ìƒì„±ê¸°ì•¼. ì£¼ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ {num_questions}ê°œì˜ {question_type} ë¬¸ì œë¥¼ í•œê¸€ë¡œ ë§Œë“¤ê³ , ê°ê° ì •ë‹µê³¼ í•´ì„¤ë„ í•¨ê»˜ ì œê³µí•´ì¤˜.
ì£¼ì œ: {topic}
"""
prompt = PromptTemplate(input_variables=["topic", "num_questions", "question_type"], template=template)
gen_chain = LLMChain(llm=llm, prompt=prompt)

# ========== 4) Streamlit ì•± UI ==========
st.set_page_config(page_title="í•œê¸€ í€´ì¦ˆ ìƒì„±ê¸°", layout="centered")
st.title("ğŸ“˜ í•œê¸€ í€´ì¦ˆ ìƒì„±ê¸° (GPT-Neo ê¸°ë°˜)")

mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ğŸ“š ê¸°ì¡´ ë¬¸ì œ ê²€ìƒ‰", "ğŸ§  ìƒˆ í€´ì¦ˆ ìƒì„±"])

# ========== 5) ê¸°ì¡´ ë¬¸ì œ ê²€ìƒ‰ ==========
if mode == "ğŸ“š ê¸°ì¡´ ë¬¸ì œ ê²€ìƒ‰":
    query = st.text_input("ğŸ” ì°¾ê³  ì‹¶ì€ í‚¤ì›Œë“œë‚˜ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    if st.button("ê²€ìƒ‰"):
        from langchain.chains import RetrievalQA
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            result = qa_chain.run(query)
        st.markdown("### âœ… ê²€ìƒ‰ ê²°ê³¼")
        st.write(result)

# ========== 6) ìƒˆ í€´ì¦ˆ ìƒì„± ==========
else:
    topic = st.text_input("ì£¼ì œ ì…ë ¥ (ì˜ˆ: ì¸ê³µì§€ëŠ¥, ë§ˆì¼€íŒ… ë“±)")
    num = st.number_input("ë¬¸ì œ ê°œìˆ˜", min_value=1, max_value=10, value=3)
    qtype = st.selectbox("ë¬¸ì œ ìœ í˜•", ["ê°ê´€ì‹", "OX", "ì£¼ê´€ì‹"])

    if st.button("í€´ì¦ˆ ìƒì„±"):
        with st.spinner("GPT-Neoê°€ ë¬¸ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            result = gen_chain.run({"topic": topic, "num_questions": num, "question_type": qtype})
        st.markdown("### ğŸ“ ìƒì„±ëœ í€´ì¦ˆ")
        st.text_area("ê²°ê³¼", result, height=400)

        if st.button("â• ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥"):
            new_doc = Document(page_content=result)
            faiss.add_documents([new_doc])
            st.success("âœ… ìƒˆë¡œìš´ í€´ì¦ˆê°€ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
